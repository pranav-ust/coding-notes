{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis in Spanglish\n",
    "\n",
    "The dataset that I used here is https://ritual-uh.github.io/sentimix2020/res.\n",
    "\n",
    "For data processing, I will convert the dataset from CONLL stuff to Huggingface stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Load the text file into a list\n",
    "with open('spanglish_trial.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    #lines is a list of strings \n",
    "    #each string in the form 'meta\\t1\\tpositive\\n'\n",
    "    #split each string by '\\t' and '\\n'\n",
    "    lines = [line.split('\\t') for line in lines]\n",
    "    #remove the '\\n' from the last element of each string\n",
    "    lines = [line[:-1] + [line[-1][:-1]] for line in lines]\n",
    "    #divide everything in batches started by meta\n",
    "    #each batch is a list of lists\n",
    "    batches = []\n",
    "    batch = []\n",
    "    for line in lines:\n",
    "        if line[0] == 'meta':\n",
    "            batches.append(batch)\n",
    "            batch = []\n",
    "        batch.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output a csv file\n",
    "import csv\n",
    "general_dic = dict()\n",
    "with open('spanglish_trial.csv', 'w', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    sentence, mood =  \"\", \"\"\n",
    "    for batch in batches:\n",
    "        if len(batch) == 0: continue\n",
    "        for piece in batch:\n",
    "            if piece[0] == 'meta':\n",
    "                mood = piece[2]\n",
    "            else:\n",
    "                sentence += piece[0] + ' '\n",
    "        general_dic[sentence[:-1]] = mood\n",
    "        sentence,mood = \"\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfile = open('spanglish_train.csv', 'w')\n",
    "testfile = open('spanglish_test.csv', 'w')\n",
    "validfile = open('spanglish_valid.csv', 'w')\n",
    "\n",
    "trainfilecsv = csv.writer(trainfile)\n",
    "testfilecsv = csv.writer(testfile)\n",
    "validfilecsv = csv.writer(validfile)\n",
    "\n",
    "import random\n",
    "\n",
    "for sentence, mood in general_dic.items():\n",
    "    rand = random.random()\n",
    "    if rand < 0.8:\n",
    "        trainfilecsv.writerow([sentence, mood])\n",
    "    elif rand < 0.9:\n",
    "        testfilecsv.writerow([sentence, mood])\n",
    "    else:\n",
    "        validfilecsv.writerow([sentence, mood])\n",
    "trainfile.close()\n",
    "testfile.close()\n",
    "validfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset into huggingface dataset and do label to id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the csv file into information apt for training\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "data_files = {'train': 'spanglish_train.csv', 'validation': 'spanglish_valid.csv', 'test': 'spanglish_test.csv'}\n",
    "dataset = load_dataset('csv', data_files=data_files, delimiter=',', column_names=['sentence', 'label'])\n",
    "label_to_id = {'positive': 1, 'negative': 0, 'neutral': 2}\n",
    "dataset = dataset.map(lambda example: {'label': label_to_id[example['label']]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stuff relevant to model\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "\n",
    "modelname = 'Twitter/twhin-bert-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(modelname, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us define a function to compute the model's accuracy and F1 score\n",
    "#These two steps will be performed by the model automatically\n",
    "#Calculate predictions for the validation set\n",
    "#predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "#Turn the logits into a prediction we can compare with the labels\n",
    "import numpy as np\n",
    "\n",
    "import evaluate\n",
    "# Eval_preds is an eval_oredicts object. \n",
    "# It contains two keys: predictions and label and it is generated by the trainer\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"test_trainer\", \n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    num_train_epochs=15.0,\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics = compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "0b8abeac7584045b85d319e014539c3688a4801feb3e39c9f8f5a2042774cf4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
